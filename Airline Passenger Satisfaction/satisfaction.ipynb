{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Passenger Satisfaction Analysis\n",
    "\n",
    "### <a href=https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction>Dataset</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216 from C header, got 232 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(df, numeric_columns):\n",
    "    def find_outliers_IQR(df, column):\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)].index\n",
    "        return outliers\n",
    "\n",
    "    outliers_indices = set()\n",
    "    for column in numeric_columns:\n",
    "        outliers_indices.update(find_outliers_IQR(df, column))\n",
    "    \n",
    "    df_no_outliers = df.drop(index=outliers_indices)\n",
    "    return df_no_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_encoders(df, categorical_columns):\n",
    "    label_encoders = {}\n",
    "    for column in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(df[column].astype(str))\n",
    "        label_encoders[column] = le\n",
    "    return label_encoders\n",
    "\n",
    "def apply_encoders(df, categorical_columns, label_encoders):\n",
    "    for column in categorical_columns:\n",
    "        le = label_encoders[column]\n",
    "        df[column] = le.transform(df[column].astype(str))\n",
    "    return df\n",
    "\n",
    "def initialize_scaler(df, numeric_columns):\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(df[numeric_columns])\n",
    "    return scaler\n",
    "\n",
    "def apply_scaler(df, numeric_columns, scaler):\n",
    "    df[numeric_columns] = scaler.transform(df[numeric_columns])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_values(df, categorical_columns, numeric_columns):\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[categorical_columns] = categorical_imputer.fit_transform(df[categorical_columns])\n",
    "    \n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    df[numeric_columns] = numeric_imputer.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, label_encoders=None, scaler=None, is_train=True):\n",
    "    df = df.drop([\"Unnamed: 0\", \"id\"], axis=1, errors='ignore')\n",
    "\n",
    "    threshold = 10\n",
    "    categorical_columns = [col for col in df.columns if df[col].nunique() < threshold and col != \"satisfaction\"]\n",
    "    numeric_columns = [col for col in df.columns if col not in categorical_columns + [\"satisfaction\"]]\n",
    "\n",
    "    # Handle outliers only if it is training data\n",
    "    if is_train:\n",
    "        df = handle_outliers(df, numeric_columns)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    if is_train:\n",
    "        label_encoders = initialize_encoders(df, categorical_columns)\n",
    "    df = apply_encoders(df, categorical_columns, label_encoders)\n",
    "\n",
    "    # Impute missing values\n",
    "    df = impute_missing_values(df, categorical_columns, numeric_columns)\n",
    "\n",
    "    # Scale numerical variables\n",
    "    if is_train:\n",
    "        scaler = initialize_scaler(df, numeric_columns)\n",
    "    df = apply_scaler(df, numeric_columns, scaler)\n",
    "\n",
    "    # Encode the target variable to 0 and 1\n",
    "    target = None\n",
    "    if 'satisfaction' in df.columns:\n",
    "        target = df['satisfaction'].apply(lambda x: 1 if x == 'satisfied' else 0)\n",
    "        df = df.drop(['satisfaction'], axis=1)\n",
    "\n",
    "    return df, target, label_encoders, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='weighted')\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    f1 = f1_score(y, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    print(\"Model:\", model_name)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data/train.csv')\n",
    "df_test = pd.read_csv('Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, label_encoders, scaler = preprocess_data(df_train, is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, _, _ = preprocess_data(df_test, label_encoders=label_encoders, scaler=scaler, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(class_weight='balanced'),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight='balanced', n_estimators=100),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Gaussian Naive Bayes\n",
      "Accuracy: 0.8357329842931938\n",
      "Precision: 0.8389220378915871\n",
      "Recall: 0.8357329842931938\n",
      "F1 Score: 0.8336894539937724\n",
      "Confusion Matrix:\n",
      " [[13276  1297]\n",
      " [ 2970  8433]]\n",
      "Training time for Gaussian Naive Bayes: 0.0587 seconds\n",
      "\n",
      "Model: Logistic Regression\n",
      "Accuracy: 0.8561364336310441\n",
      "Precision: 0.8561692310140316\n",
      "Recall: 0.8561364336310441\n",
      "F1 Score: 0.8555825490151608\n",
      "Confusion Matrix:\n",
      " [[13036  1537]\n",
      " [ 2200  9203]]\n",
      "Training time for Logistic Regression: 0.4039 seconds\n",
      "\n",
      "Model: K-Nearest Neighbors\n",
      "Accuracy: 0.9136510625192485\n",
      "Precision: 0.9147830962723809\n",
      "Recall: 0.9136510625192485\n",
      "F1 Score: 0.9132011216880185\n",
      "Confusion Matrix:\n",
      " [[13878   695]\n",
      " [ 1548  9855]]\n",
      "Training time for K-Nearest Neighbors: 0.0157 seconds\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.941137973514013\n",
      "Precision: 0.9411444691086673\n",
      "Recall: 0.941137973514013\n",
      "F1 Score: 0.9411410503472337\n",
      "Confusion Matrix:\n",
      " [[13803   770]\n",
      " [  759 10644]]\n",
      "Training time for Decision Tree: 0.4793 seconds\n",
      "\n",
      "Model: Random Forest\n",
      "Accuracy: 0.9630428087465353\n",
      "Precision: 0.9632923977359493\n",
      "Recall: 0.9630428087465353\n",
      "F1 Score: 0.9629701492493691\n",
      "Confusion Matrix:\n",
      " [[14277   296]\n",
      " [  664 10739]]\n",
      "Training time for Random Forest: 7.7222 seconds\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Accuracy: 0.9429088389282414\n",
      "Precision: 0.9429724739637796\n",
      "Recall: 0.9429088389282414\n",
      "F1 Score: 0.9428233782135655\n",
      "Confusion Matrix:\n",
      " [[13975   598]\n",
      " [  885 10518]]\n",
      "Training time for Gradient Boosting: 10.3515 seconds\n",
      "\n",
      "Model: XGBoost\n",
      "Accuracy: 0.9628118263012011\n",
      "Precision: 0.9630337073449199\n",
      "Recall: 0.9628118263012011\n",
      "F1 Score: 0.9627422196762695\n",
      "Confusion Matrix:\n",
      " [[14266   307]\n",
      " [  659 10744]]\n",
      "Training time for XGBoost: 0.3767 seconds\n",
      "\n",
      "Model: MLP Classifier\n",
      "Accuracy: 0.9538035109331691\n",
      "Precision: 0.9537869456978199\n",
      "Recall: 0.9538035109331691\n",
      "F1 Score: 0.9537802929824517\n",
      "Confusion Matrix:\n",
      " [[14024   549]\n",
      " [  651 10752]]\n",
      "Training time for MLP Classifier: 86.4713 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/canetizen/.local/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: AdaBoost\n",
      "Accuracy: 0.9266245765321836\n",
      "Precision: 0.9265789924049591\n",
      "Recall: 0.9266245765321836\n",
      "F1 Score: 0.9265633467272038\n",
      "Confusion Matrix:\n",
      " [[13703   870]\n",
      " [ 1036 10367]]\n",
      "Training time for AdaBoost: 2.6899 seconds\n",
      "\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 37582, number of negative: 47787\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 485\n",
      "[LightGBM] [Info] Number of data points in the train set: 85369, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.440230 -> initscore=-0.240228\n",
      "[LightGBM] [Info] Start training from score -0.240228\n",
      "Model: LightGBM\n",
      "Accuracy: 0.9633122882660918\n",
      "Precision: 0.96363453287548\n",
      "Recall: 0.9633122882660918\n",
      "F1 Score: 0.9632316012284768\n",
      "Confusion Matrix:\n",
      " [[14300   273]\n",
      " [  680 10723]]\n",
      "Training time for LightGBM: 0.3042 seconds\n",
      "\n",
      "Model: CatBoost\n",
      "Accuracy: 0.9637357560825377\n",
      "Precision: 0.9639815014410058\n",
      "Recall: 0.9637357560825377\n",
      "F1 Score: 0.963665316659757\n",
      "Confusion Matrix:\n",
      " [[14284   289]\n",
      " [  653 10750]]\n",
      "Training time for CatBoost: 9.7227 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    evaluate_model(model_name, model, X_test, y_test)\n",
    "    print(f\"Training time for {model_name}: {training_time:.4f} seconds\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
